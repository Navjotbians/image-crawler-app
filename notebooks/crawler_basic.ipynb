{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE CRAWLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import *\n",
    "from queue import Queue\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "from threading import Thread\n",
    "import re\n",
    "from flask import abort, Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://www.airbnb.co.uk/s/Ljubljana--Slovenia/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&query=Ljubljana%2C%20Slovenia&place_id=ChIJ0YaYlvUxZUcRIOw_ghz4AAQ&checkin=2020-11-01&checkout=2020-11-08&source=structured_search_input_header&search_type=autocomplete_click\"\n",
    "# [\"http://4chan.org/\",\"http://golang.org/\"]\n",
    "# url2 = \"http://4chan.org/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='scraper - %(asctime)s - %(name)s - %(threadName)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "app = Flask(__name__)\n",
    "tasks_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks:\n",
    "    \n",
    "    def __init__(self, task_uuid, queue_urls):\n",
    "        from time import time\n",
    "        self.found_urls = {}\n",
    "        self.urls_done = {}\n",
    "        self.time_takes = None\n",
    "        self.time_start = time()\n",
    "        self.task_uuid = task_uuid\n",
    "        for url in queue_urls:\n",
    "            self.urls_done[url] = False\n",
    "            self.found_urls[url] = []\n",
    "\n",
    "    def prepare_url(self, src_url, url):\n",
    "        import re\n",
    "        if re.match(r'^[/]{2}.*', url):\n",
    "            return f'http:{url}'\n",
    "        elif re.match(r'^/\\w+.*', url):\n",
    "            return f'{src_url}{url}'\n",
    "        elif not re.match(r'^http.*', url):\n",
    "            return f'{src_url}/{url}'\n",
    "        return url\n",
    "\n",
    "    def _add_url(self, src_url, url):\n",
    "        url_needed = False\n",
    "        for pat in ['.gif', '.jpg', '.png']:\n",
    "            if pat in url:\n",
    "                url_needed = True\n",
    "                break\n",
    "        if url_needed:\n",
    "            if url not in self.found_urls[src_url]:\n",
    "                self.found_urls[src_url].append(url)\n",
    "\n",
    "    def add_res(self, src_url, urls):\n",
    "        if isinstance(urls, str):\n",
    "            self._add_url(src_url, urls)\n",
    "        else:\n",
    "            for url in urls:\n",
    "                self._add_url(src_url, url)\n",
    "\n",
    "    def finished(self, url):\n",
    "        from time import time\n",
    "        self.urls_done[url] = True\n",
    "        self.time_takes = time() - self.time_start\n",
    "        logging.info(f'{self.task_uuid} Took {self.time_takes}')\n",
    "\n",
    "    def status(self):\n",
    "        return {\"completed\": sum(self.urls_done.values()),\n",
    "                \"inprogress\": len(self.urls_done.values()) - sum(self.urls_done.values())}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_scraping(task_id, url_list, threads=1):\n",
    "    from queue import Queue\n",
    "    task = Tasks(task_id, url_list)\n",
    "    tasks_results[task_id] = task\n",
    "\n",
    "    def worker(task_queue, task):\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        ua = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0'\n",
    "        }\n",
    "\n",
    "        def parse_url(src_url, url, deep_lv=1):\n",
    "            try:\n",
    "                r = requests.get(url, headers=ua)\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                hrefs = soup.find_all(href=True)\n",
    "                images = soup.findAll('img')\n",
    "                task.add_res(src_url, [task.prepare_url(url, link['src']) for link in images])\n",
    "\n",
    "                if deep_lv > 0:\n",
    "                    for href in hrefs:\n",
    "                        parse_url(src_url, task.prepare_url(url, href['href']), deep_lv=deep_lv - 1)\n",
    "            except:\n",
    "                logger.warning(f'Cant go by link {url}')\n",
    "                pass\n",
    "\n",
    "        while True:\n",
    "            url = task_queue.get()\n",
    "            if url is None:\n",
    "                break\n",
    "            parse_url(url, url)\n",
    "            task.finished(url)\n",
    "            task_queue.task_done()\n",
    "\n",
    "    queue = Queue()\n",
    "    for x in range(threads):\n",
    "        worker = Thread(target=worker, name=f\"{task_id}_{x}\", args=(queue, task))\n",
    "        # Setting daemon to True will let the main thread exit even though the workers are blocking\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "    # Put the tasks into the queue\n",
    "    for link in url_list:\n",
    "        logger.info('{} Queueing {}'.format(task_id, link))\n",
    "        queue.put(link)\n",
    "    # Add flag break threads\n",
    "    for x in range(threads):\n",
    "        queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/status/<task_uuid>', methods=['GET'])\n",
    "def return_status(task_uuid):\n",
    "    if task_uuid in tasks_results:\n",
    "        return jsonify(tasks_results[task_uuid].status())\n",
    "    else:\n",
    "        abort(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/statistics', methods=['GET'])\n",
    "def return_statistics():\n",
    "    info = {'tasks': len(tasks_results.keys()),\n",
    "            'urls_requseted': sum([len(task.urls_done.keys()) for task_uuid, task in tasks_results.items()]),\n",
    "            'tasks_ids': {}}\n",
    "    for task_uuid, task in tasks_results.items():\n",
    "        info['tasks_ids'][task_uuid] = len(task.urls_done.keys())\n",
    "    return jsonify(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "@app.route('/result/<task_uuid>', methods=['GET'])\n",
    "def return_result(task_uuid):if task_uuid in tasks_results:\n",
    "        return jsonify(tasks_results[task_uuid].found_urls)\n",
    "    else:\n",
    "        abort(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['POST'])\n",
    "def get_task():\n",
    "    if not request.json:\n",
    "        abort(400)\n",
    "    urls = request.json\n",
    "    if not isinstance(urls, list):\n",
    "        abort(400)\n",
    "    taks_uuid = str(uuid.uuid1())\n",
    "    thread_n = 1\n",
    "    start_scraping(taks_uuid, urls, thread_n)\n",
    "    result = {\"job_id\": taks_uuid, \"threads\": str(thread_n), \"urls\": urls}\n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    port = int(os.environ.get('PORT', 5000))\n",
    "    app.run(threaded=False, host='0.0.0.0', port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
