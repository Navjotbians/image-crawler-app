{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE CRAWLER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. An application must take the form of a Web API server.\n",
    "2. We expect that the app will take a list of URLs as input and a number of threads,\n",
    "and return a job ID used to fetch crawling information/results.\n",
    "3. The app should be able to run multiple crawling jobs at the same time.\n",
    "4. We expect to be able to get information/results about a running/finished job\n",
    "from your application (using its job ID).\n",
    "5. The data extracted will be a list of image URLs (gif, jpg, png) as output.\n",
    "6. The app should crawl the URLs recursively only until the second level\n",
    "(to avoid a large amount of data): Fetch the images for each given URL and their children.\n",
    "7. By default, the app will crawl using one thread/coroutine, however,\n",
    "we should be able to specify the number of thread/coroutine to use when creating a new job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import *\n",
    "from queue import Queue\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "from threading import Thread\n",
    "import re\n",
    "from flask import abort, Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://www.airbnb.co.uk/s/Ljubljana--Slovenia/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&query=Ljubljana%2C%20Slovenia&place_id=ChIJ0YaYlvUxZUcRIOw_ghz4AAQ&checkin=2020-11-01&checkout=2020-11-08&source=structured_search_input_header&search_type=autocomplete_click\"\n",
    "# [\"http://4chan.org/\",\"http://golang.org/\"]\n",
    "# url2 = \"http://4chan.org/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='scraper - %(asctime)s - %(name)s - %(threadName)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "app = Flask(__name__)\n",
    "tasks_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks:\n",
    "    \n",
    "    def __init__(self, task_uuid, queue_urls):\n",
    "        from time import time\n",
    "        self.found_urls = {}\n",
    "        self.urls_done = {}\n",
    "        self.time_takes = None\n",
    "        self.time_start = time()\n",
    "        self.task_uuid = task_uuid\n",
    "        for url in queue_urls:\n",
    "            self.urls_done[url] = False\n",
    "            self.found_urls[url] = []\n",
    "\n",
    "    def prepare_url(self, src_url, url):\n",
    "        import re\n",
    "        if re.match(r'^[/]{2}.*', url):\n",
    "            return f'http:{url}'\n",
    "        elif re.match(r'^/\\w+.*', url):\n",
    "            return f'{src_url}{url}'\n",
    "        elif not re.match(r'^http.*', url):\n",
    "            return f'{src_url}/{url}'\n",
    "        return url\n",
    "\n",
    "    def _add_url(self, src_url, url):\n",
    "        url_needed = False\n",
    "        for pat in ['.gif', '.jpg', '.png']:\n",
    "            if pat in url:\n",
    "                url_needed = True\n",
    "                break\n",
    "        if url_needed:\n",
    "            if url not in self.found_urls[src_url]:\n",
    "                self.found_urls[src_url].append(url)\n",
    "\n",
    "    def add_res(self, src_url, urls):\n",
    "        if isinstance(urls, str):\n",
    "            self._add_url(src_url, urls)\n",
    "        else:\n",
    "            for url in urls:\n",
    "                self._add_url(src_url, url)\n",
    "\n",
    "    def finished(self, url):\n",
    "        from time import time\n",
    "        self.urls_done[url] = True\n",
    "        self.time_takes = time() - self.time_start\n",
    "        logging.info(f'{self.task_uuid} Took {self.time_takes}')\n",
    "\n",
    "    def status(self):\n",
    "        return {\"completed\": sum(self.urls_done.values()),\n",
    "                \"inprogress\": len(self.urls_done.values()) - sum(self.urls_done.values())}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_scraping(task_id, url_list, threads=1):\n",
    "    from queue import Queue\n",
    "    task = Tasks(task_id, url_list)\n",
    "    tasks_results[task_id] = task\n",
    "\n",
    "    def worker(task_queue, task):\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        ua = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        def parse_url(src_url, url, deep_lv=1):\n",
    "            try:\n",
    "                r = requests.get(url, headers=ua)\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                hrefs = soup.find_all(href=True)\n",
    "                images = soup.findAll('img')\n",
    "                task.add_res(src_url, [task.prepare_url(url, link['src']) for link in images])\n",
    "\n",
    "                if deep_lv > 0:\n",
    "                    for href in hrefs:\n",
    "                        parse_url(src_url, task.prepare_url(url, href['href']), deep_lv=deep_lv - 1)\n",
    "            except:\n",
    "                logger.warning(f'Cant go by link {url}')\n",
    "                pass\n",
    "\n",
    "        while True:\n",
    "            url = task_queue.get()\n",
    "            if url is None:\n",
    "                break\n",
    "            parse_url(url, url)\n",
    "            task.finished(url)\n",
    "            task_queue.task_done()\n",
    "\n",
    "    queue = Queue()\n",
    "    for x in range(threads):\n",
    "        worker = Thread(target=worker, name=f\"{task_id}_{x}\", args=(queue, task))\n",
    "        # Setting daemon to True will let the main thread exit even though the workers are blocking\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "    # Put the tasks into the queue\n",
    "    for link in url_list:\n",
    "        logger.info('{} Queueing {}'.format(task_id, link))\n",
    "        queue.put(link)\n",
    "    # Add flag break threads\n",
    "    for x in range(threads):\n",
    "        queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/status/<task_uuid>', methods=['GET'])\n",
    "def return_status(task_uuid):\n",
    "    if task_uuid in tasks_results:\n",
    "        return jsonify(tasks_results[task_uuid].status())\n",
    "    else:\n",
    "        abort(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/statistics', methods=['GET'])\n",
    "def return_statistics():\n",
    "    info = {'tasks': len(tasks_results.keys()),\n",
    "            'urls_requseted': sum([len(task.urls_done.keys()) for task_uuid, task in tasks_results.items()]),\n",
    "            'tasks_ids': {}}\n",
    "    for task_uuid, task in tasks_results.items():\n",
    "        info['tasks_ids'][task_uuid] = len(task.urls_done.keys())\n",
    "    return jsonify(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/result/<task_uuid>', methods=['GET'])\n",
    "def return_result(task_uuid):\n",
    "    if task_uuid in tasks_results:\n",
    "        return jsonify(tasks_results[task_uuid].found_urls)\n",
    "    else:\n",
    "        abort(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['POST'])\n",
    "def get_task():\n",
    "    if not request.json:\n",
    "        abort(400)\n",
    "    urls = request.json\n",
    "    if not isinstance(urls, list):\n",
    "        abort(400)\n",
    "    taks_uuid = str(uuid.uuid1())\n",
    "    thread_n = 1\n",
    "    start_scraping(taks_uuid, urls, thread_n)\n",
    "    result = {\"job_id\": taks_uuid, \"threads\": str(thread_n), \"urls\": urls}\n",
    "    return jsonify(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scraper - 2021-05-01 03:12:25,461 - werkzeug - MainThread - INFO -  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "port = int(os.environ.get('PORT', 5000))\n",
    "app.run(threaded=False, host='0.0.0.0', port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
